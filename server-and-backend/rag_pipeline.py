# -*- coding: utf-8 -*-
"""rag_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16QTn8jtIweHGV1sQ29Fr0Ah6g_3TCzux
"""

import os
import sys
from typing import List, Dict # type 정하기

# .env 파일 로드(API Key 보안)
from dotenv import load_dotenv
load_dotenv()

# LangChain, Google AI 임포트
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


# 초기화 작업 (서버 시작 시 1회만 실행)
# 서버 과부하 방지. 검색 속도 향상 목적. 모든 준비 마친 검색기 리턴.
def prepare_rag_engine(pdf_path: str):
    # PDF 로드
    try:
        loader = PyPDFLoader(file_path=pdf_path)
        raw_docs = loader.load()
        print(f"PDF 로드 성공: 총 {len(raw_docs)} 페이지.")
    except Exception as e:
        print(f"PDF 파일을 찾을 수 X.: {e}")
        return None

    # 청크 분할
    if raw_docs:
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        docs = text_splitter.split_documents(raw_docs)
        print(f"{len(docs)}개의 문서 조각 생성.")
    else:
        print("PDF 비어 있음. 청크 실패.")
        return None

    # 벡터 DB 및 검색기 생성
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

        # 벡터 DB 생성 및 저장
        db = FAISS.from_documents(docs, embeddings)

        # 검색기 세팅
        retriever = db.as_retriever(search_kwargs={"k": 10})

        print(f"벡터 DB 및 검색기 생성 완료.")
        return retriever

    except Exception as e:
        print(f"벡터 DB 생성 실패.: {e}")
        return None

# 서버 구동 인원의 로컬에 저장된 경로
PDF_FILE_PATH = r"C:\capstone\backend\real_used_in_RAG_policy.pdf"

# g_는 전역변수를 의미함.
g_retriever_pdf = prepare_rag_engine(PDF_FILE_PATH)

# LLM 및 프롬프트 초기화.
try:
    g_llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro")

    prompt_template = """
    당신은 전문적인 '여행 컨설턴트'입니다.
    아래 제공된 [컨텍스트]을 바탕으로, 고객을 위한 '여행 추천 보고서'를 작성하십시오.

    [작성 규칙]
    1. 반드시 아래 [컨텍스트]에 포함된 상품과 정보만을 근거로 작성해야 합니다.
    2. 추천된 각 상품이 '왜' 좋은지, PDF 상세 설명을 인용하여 설득력 있게 설명하십시오.
    3. 보고서 톤은 정중하고 전문적으로 작성하십시오.

    [컨텍스트]

    ### 서버 추천 상품 (핵심 정보)
    {server_product_context}

    ###  pdf 부연 설명.
    {pdf_description_context}

    [질문]
    {question}
    """

    # LangChain이 이해할 수 있는 프롬프트 양식으로 변환.
    g_prompt = ChatPromptTemplate.from_template(prompt_template)

    # 생성 체인 구성. 상품명 개수만큼 검색해야 하므로 검색기는 해당 체인서 제외.
    g_chain = g_prompt | g_llm | StrOutputParser()
    print("체인 준비 완료.\n")

except Exception as e:
    print(f"LLM 초기화 실패: {e}")
    g_chain = None

# 서버가 호출할 함수. Dict으로 구성된 List가 인풋. str가 아웃풋.
def create_report(product_data_list: List[Dict]) -> str:
    # 안전 장치
    if not g_retriever_pdf or not g_chain:
        return "RAG 엔진이 초기화되지 않았습니다. 서버 로그를 확인해주세요."

    if not product_data_list:
        return "추천할 상품 데이터가 없습니다."

    print(f"\n보고서 생성 요청 받음 (상품 {len(product_data_list)}개)")

    # PDF서 부연 설명 (수동 Loop)
    # 상품명에 대한 설명 담을 리스트.
    pdf_descriptions_list = []

    # 상품명 개수만큼 실행.
    for product in product_data_list:
        # 딕셔너리에서 'product_name' 추출 (키 이름이 다르면 수정 필요)
        p_name = product.get("product_name", "이름 XX.")
        print(f"  - '{p_name}' 상세 정보 검색 ", end="")

        try:
            # 검색어(상품명)로 PDF 검색기 호출. 검색 결과를 저장.
            retrieved_chunks = g_retriever_pdf.invoke(p_name)

            if retrieved_chunks:
                # 검색된 청크들을 하나의 텍스트로 합침.
                combined_text = "\n".join([doc.page_content for doc in retrieved_chunks])
                pdf_descriptions_list.append(f"상품명: {p_name}\n[상세 설명]:\n{combined_text}\n")
            else:
                pdf_descriptions_list.append(f"상품명: {p_name}\n(상세 설명 정보를 찾을 수 X)\n")
                print("실패.")

        except Exception as e:
            print(f" 오류 발생: {e}")
            pdf_descriptions_list.append(f"상품명: {p_name}\n(검색 중 오류 발생)\n")

    # 서버가 준 상품 정보 (문자열로 변환)
    server_context_str = ""
    for p in product_data_list:
        server_context_str += f"- {p}\n"

    # PDF 부연 설명 (문자열로 변환)
    pdf_context_str = "\n".join(pdf_descriptions_list)

    # 최종 입력값 구성.
    final_inputs = {
        "server_product_context": server_context_str,
        "pdf_description_context": pdf_context_str,
        "question": "위 정보를 종합하여 고객에게 최적의 여행 계획을 제안하는 보고서를 작성해줘."
    }

    # LLM 호출
    report_text = g_chain.invoke(final_inputs)

    return report_text